---
layout: post
title:  "Async I/O in Zig"
date:   2025-01-11 19:39:00 -0800
---

I've recently finished a deep-dive into the various APIs that are available for
[Asynchronous I/O](https://en.wikipedia.org/wiki/Asynchronous_I/O) on different
platforms. Specifically, [io_uring](https://en.wikipedia.org/wiki/Io_uring) on
Linux, and [kqueue](https://en.wikipedia.org/wiki/Kqueue) on macOS / BSDs.
Windows uses [IOCP](https://en.wikipedia.org/wiki/Input/output_completion_port),
though I haven't yet dug into this as I focused on first supporting Linux and
macOS. This blog post is meant as a summary of what I've learned, and in
general, will try to go deep on how the various async I/O interfaces work on
different platforms.

#### What is Async I/O?

An I/O operation (i.e. reading from a socket, writing to a file) can be either
synchronous or asynchronous. The former is the typical API we're all used to,
i.e. [read(2)](https://man7.org/linux/man-pages/man2/read.2.html) or
[send(2)](https://man7.org/linux/man-pages/man2/send.2.html). A synchronous API
is blocking, meaning that the calling thread is blocked (suspended) until the
operation finishes. In general, synchronous I/O means simpler, easier to
understand code, and as such, it's probably the right choice for 95% of
applications. In fact, reaching for async I/O too quickly is probably a
net-negative in a lot of cases -- [computers are
fast](https://computers-are-fast.github.io).

That said, especially in high-performance servers where throughput is paramount,
sometimes reaching for a more powerful tool is justified. The easy next step
without learning any new APIs or adding as much complexity to your system is to
dispatch work to an I/O threadpool, i.e. at the start of your program, spin up a
bunch of threads that are waiting for I/O requests from your program. When one
of these worker threads get a request, they themselves can block on the syscall,
the rest of your program can keep working on other stuff, and once the I/O is
done, your program can be notified of the result. This is the basic premise of
all asynchronous APIs, i.e. the ability to queue/schedule I/O work to be done,
keep doing other stuff in the meantime, then get told once that work has been
completed.

This pattern of queuing work and getting notified once its done is so common
and powerful that all major operating systems ship with purpose-built APIs for
async I/O. Specifically, an async system can, when designed properly, handle
massive throughput as application threads can keep doing work on the CPU while
waiting for data to be sent or received.

#### Modern concurrency models

Most modern programming languages, i.e. Go, Rust, JS, have the concept of
asynchronous operations built in as a core primitive of the language, often time
hidden from the application entirely. This means that developers can leverage
these powerful platform-specific APIs without needing to know much about how
they're implemented.

One of the most common "async abstractions" is `async/await`. Consider the
following bit of Rust code (taken from the [Rust Async
Book](https://rust-lang.github.io/async-book/part-guide/async-await.html)):

```rust
#[tokio::main]
async fn main() -> Result<()> {
    // Open a connection to the mini-redis address.
    let mut client = client::connect("127.0.0.1:6379").await?;

    // Set the key "hello" with value "world"
    client.set("hello", "world".into()).await?;

    // Get key "hello"
    let result = client.get("hello").await?;

    println!("got value from the server; result={:?}", result);

    Ok(())
}
```

Each one of those `await` calls are tacked on to a [Rust
Future](https://doc.rust-lang.org/std/future/trait.Future.html), a
language-primitive for representing an ongoing, possibly not finished
computation. By `await`'ing the future, the Rust program is waiting (blocking)
for the result of an asynchronous operation. The neat thing is that this means
it's possible to [spawn](https://docs.rs/tokio/latest/tokio/task/fn.spawn.html)
a some work as a Future, keep doing other work, and then `await` that Future
when the application is ready to consume the value. Behind the scenes, the
underlying async runtime (i.e. [tokio](https://tokio.rs)) is queuing up I/O
operations with the operating system, and waking up tasks (i.e. bits of
application code) when those operations complete.

Go uses a particularly clever approach to concurrency, which is pretty similar
to modern async Rust, but built into the language from day 1. All go "threads"
are called goroutines, and are multiplexed onto operating threads by the Go
runtime. The programmer can call `file.Read()` with no syntactic sugar, and
behind the scenes, Go will queue that read with the OS and keep doing work on
other goroutines. When the operating system completes the read, the original
goroutine will be marked as runnable and the runtime will start it up again to
keep making progress.

I'm a really big fan of both Go's transparent async approach as well as the
`async/await` implementations in languages like Rust and JS. It's a really
clever model for hiding a ton of complexity from the programmer, i.e. allowing
them to write code that appears to have easy to understand, linear control flow,
while also minimizing the amount of time the CPU is spent blocked / not doing
any meaningful work.

However, and this is the fun bit, there isn't any `async/await` in Zig -- there
was, but it was removed from the language. Eventually, it'll be probably added
back to the language, but in the meantime, does this mean we can't write
properly asynchronous systems? Absolutely not! We'll just do it from scratch!

#### Giving credit where it's due

Before continuing, I wanted to make clear that the following isn't original work
by any means, and by writing this, I'm very blatently standing on the shoulders
of some pretty-amazing giants.

- [Tigerbeetle's Async I/O
  Abstraction](https://tigerbeetle.com/blog/2022-11-23-a-friendly-abstraction-over-iouring-and-kqueue/)
  by King Butcher and Phil Eaton
- [libxev](https://github.com/mitchellh/libxev) by Mitchell Hashimoto

In general, the idea is simple -- we need to be able to submit operations to the
kernel, in a platform-agnostic way, and then get a callback when the work is
completed. In the meantime, we can keep on doing other stuff.

For example, accepting a new client connection to a server socket looks like:

```zig
const completion: Completion = .{
    .operation = .{
        .accept = .{
            .socket = self.socket,
        },
    },
    .context = &server,
    .callback = acceptCallback,
};
event_loop.submit(&completion);

...

fn acceptCallback(
    ctx: ?*anyopaque,
    loop: *IO.Loop,
    cpl: *IO.Completion,
    result: IO.Result,
) void {
    const server: *Server = @ptrCast(@alignCast(ctx.?));
    const client_socket = result.accept catch |err| {
        // Handle error
    };
    
    // Handle the client connection
}
```

Specifically, a `Completion` object here is to represent the state of a pending
operation. Once completed, the result along with an optional bit of state
(usually a pointer) is passed to a callback function. This is essentially what
Rust/Go does, but without any of the syntactic sugar -- we're on our own here.

#### Callback-driven programming

Without `async/await` or the semi-magical Go runtime, as we saw above, we need
to write the callback functions ourselves. This style of programming is
something that frankly, can be a little bit tricky to reason about. Rather than
simple, linear control flow (i.e. `read()` -> `write()` etc), we have sequencies
of work modeled as a series of callback functions, with each invocation mutating
some sort of state machine.

As an example, let's write an async HTTP server in Zig with callback functions.
To keep things simple and understandable, this is going to be pseudocode, but
the real implementation isn't too much more than this.

```zig
struct Server { 
    fn start(self: *Server) {
        queueAccept();
    }

    fn onAccept() {
       const connection = ...;
       connection.init(socket);
       queueAccept();
    }
}

struct Connection {
    fn init(self: *Connection, socket: posix.socket_t) void {
        self.socket = socket;
        queueRead(socket);
    }

    fn onRead() {
        parseHeaders();
        const response = processResponse();
        queueWrite(response);
    }

    fn onWrite() {
        if (keep_alive) {
            queueRead(); // Start the cycle all over again
        } else {
            queueClose();
        }
    }

    fn onClose() {
        // Cleanup memory etc.
    }
}
```

Specifically, what's happening here:
- The server accepts new connections in essentially a loop, i.e. when a new
  connection is accepted, the server will re-queue another accept.
- Connections themselves are modeled as a sequence of `onRead()` callbacks being
  fired followed by a sequence of `onWrite()` callbacks. Finally, once the
  connection is finished, a close operation is queued.

In my implementation of async I/O, this basic server design, running on a single
server thread, [benchmarked extremely
well](https://x.com/morgallant/status/1876050649697382580):

- macOS (kqueue): ~229.92k requests per second
- Linux (io_uring): ~249.85k requests per second

For context, the equivalent Go program (running with `GOMAXPROCS=1`) benchmarked
at ~115.73k requests per second -- we're roughly twice as fast. That said,
we're comparing a complete, production-grade web server against the absolutely
most-barebone server implementation possible, so we shouldn't try to draw too
much from these benchmarks. I included this comparison moreso to provide a
decent point of reference, and frankly, to pay a bit of respect. Go being able
to handle ~115k requests per second in such a high-language, providing as good
of a developer experience as it does, while also implementing all the bells and
whistles of a real web server, is extremely impressive!

#### kqueue internals

Alright, let's get into the fun part. How do these async operating system APIs
work under the hood? Well, let's start with macOS, as it's a bit easier to
reason about, then we'll move onto the beast that is io_uring.

One thing I didn't mention above is that synchronous syscall APIs can typically
be configured to run in non-blocking mode. For example: 

```zig
const rc = posix.system.pread(..);
return switch (posix.errno(rc)) {
    .SUCCESS => { // Use result },
    .AGAIN => {
        // This operation would block! But the OS knew this,
        // so instead of blocking, it instead told us about it.
    },
};
```

If we get the `AGAIN` errno, it means we need to try the operation later.. but
when do we try again? In a few seconds? An hour? One way to solve this problem
would be to poll every once and a while, i.e. keep trying every little bit until
the operation goes through. This works, but wastes valuable CPU time .. wouldn't
it be great if macOS could just tell us when things are ready?

kqueue() to the rescue! By submitting kqueue events, we can ask the kernel to
tell us when certain operations are ready, i.e. won't block if we try to do
them. So, when we get the `AGAIN` above, we just need to submit a kqueue event
to the kernel:

```zig
const event: posix.Kevent = .{
    .ident = @as(u32, @intCast(fd)),
    .filter = @as(u32, @intCast(posix.system.EVFILT_READ)),
    .flags = posix.system.EV_ADD |
        posix.system.EV_ENABLE | posix.system.EV_ONESHOT,
    .fflags = 0,
    .data = 0,
    .udata = user_data_ptr,
};

var finished_events: []posix.Kevent = ...;
try posix.kevent(
    self.kq,
    &[_]posix.Kevent{event}, // Submitting these events
    &finished_events,
    timeout,
);

for (finished_events) |event| {
    // Process any work that is now ready to be done
}
```


Notably, the
[kevent](https://man.freebsd.org/cgi/man.cgi?query=kevent&manpath=FreeBSD+9.0-RELEASE)
syscall both lets us submit new events to the kernel, as well as gives us a
number of events back that have been completed. More specifically, once we get
the event back from `kqueue()`, it's ready to be performed, i.e. we can perform
the operation again and it won't block this time.

To be honest, that's kind of.. it. Easy right? To summarize, here's how we do a
`read()` in the kqueue world:

- Try doing the read, i.e. `posix.system.pread(..)`
- If that succeeds, great, we're done.
- Otherwise, if the kernel tells us that the operation would block
    - Submit the event to kqueue()
    - Keep calling kqueue() (possibly submitting other events too) until we get
      notified that the read is ready
    - Do the read, it shouldn't block, done!

The nice thing is that the same logic works for all kind of async operations,
i.e. read, write, send, recv, accept, connect.

#### io_uring internals

io_uring is a a lot newer than kqueue; it was originally released in ~2019 as
part of the Linux kernel by [Jens
Axboe](https://en.wikipedia.org/wiki/Jens_Axboe). I'll be giving a decently
high-level overview of io_uring as it pertains to implementing core async I/O
operations in Zig, but for those interested, Jens wrote a detailed paper on the
system that goes into a lot more depth
([io_uring.pdf](https://kernel.dk/io_uring.pdf))!

If we take apart the sequence of syscalls needed for a kqueue() operation, we
see that in general, for a given operation, an application will need to do ~4
syscalls. The original op (i.e. `read()`), a submission `kqueue()`, a `kqueue()`
to get the event back from the kernel, and finally, the `read()` that won't
block. The `kqueue()` syscall cost is ammortized with other ongoing operations
(i.e. you can submit / receive multiple events at a time), but fundamentally,
it's a lot of syscalls.. and syscalls are (relatively) expensive.

io_uring was designed first and foremost for efficency, and as such, for the
same `read()` operation as above, only requires two syscalls, the first to
submit the work to the kernel, the second to receive the result (and both of
these work on multiple events at a time, i.e. the cost of the syscalls is
ammortized).

Without going into too much detail, io_uring fundamentally operates over two
data structures, `io_uring_sqe` and `io_uring_cqe`. These are the data
structures that represent a pending operation to be performed by the kernel, and
the result of said operation, respectively.

Let's do a read with io_uring:

```zig
var ring = try io_uring.init(..);

// Get an SQE and configure it to read from a specific file at an offset.
var sqe: io_uring_sqe = try ring.get_sqe();
sqe.prep_read(
    file_descriptor,
    buffer,
    offset,
);

// Submit the SQEs to the kernel.
try ring.submit();

// ...

var cqes: [256]io_uring_cqe: undefined;
const num_completed = try ring.copy_cqes(&cqes, 0);
for (cqes[0..num_completed]) |cqe| {
    // Operation is finished, process the result
}
```

A few notable differences from kqueue:
- io_uring directly passes back the result of the operation (and will write data
  to application-provided buffers directly), rather than the application having
  to do the `read()` syscall again manually getting notified -- much more
  efficient.
- In general, more of the work itself is being handled by io_uring, and thus its
  a much more heavy-duty API (kqueue is essentially a nicely ammortized poll).

In general, io_uring definitely maps better to how I'd expect an async I/O API
to work, i.e. there's a clear concept of submit -> get completions that isn't
present in kqueue().

#### Wrapping things up with a bow

Of course, this blog post wouldn't be complete without me shilling Zig a bit. If
we design both our kqueue() and io_uring implementations to expose the same API,
we can use a Zig "comptime interface" to expose them to the rest of our
application in a platform-agnostic way:

```
pub const kqueue = @import("io/kqueue.zig");
pub const io_uring = @import("io/io_uring.zig");

pub const IO = switch (builtin.target.os.tag) {
    .macos, .tvos, .watchos, .ios => kqueue,
    .linux => io_uring,
    else => @compileError("io not supported on this platform"),
};
```

A few notes:
- Since `builtin.target.os.tag` is known at compile-time, Zig will evaluate it
  and only compile the implementation that matches the target platform. For
  example, if we're compiling for macOS, the code for the io_uring
  implementation a) won't be analyzed by the Zig compiler and b) won't be
  included in the resulting binary.
- No runtime overhead whatsoever, no dynamic dispatch!

This platform-agnostic async I/O primitive is super powerful, i.e. now anything
in our app can submit events and get notified about their completion by the
kernel. Essentially, we've turned our entire app into a giant state machine. Our
`main()` function drives that state machine:

```zig
fn main() !void {
    var loop = try IO.Loop.init(..);

    // ...

    while (!shutting_down) {
        try loop.tick(); // Queue pending events, fire callbacks if any
    }

    // ...
}
```

Not a bad way to write software!
